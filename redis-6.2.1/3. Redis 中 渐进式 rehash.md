# Redis 中 渐进式 rehash

redis 底层的 hash 可能会存大量的数据，当 hash 中的 数据负载(负载因子) 到达一定阈值时，会触发 rehash 的 过程：

>大体流程就是新创建一个 hash 对象(new-hash)，然后把老的 hash 对象(old-hash) 中的所有数据插入到 new-hash 里边，然后删除 old-hash。

问题在于 redis 是单线程机制的，当 hash 表中的数据量太大，如果阻塞执行所有数据的 rehash 过程，势必会造成服务的暂时不可用。

但是 redis 使用了渐进式 rehash 的办法，来解决这个问题。

<br>

## 1. 渐进式 rehash 的基本思路
----

1. 每个dict有两个table， 其中table[0],是主table (在rehash的过程中属于 old-data 的table)

2. 当需要rehash的时候，table中的 rehashidx 会置成非负

3. 当每次调用find，add，等的操作的时候，会先执行一次rehash，当old-table中的数据都移到new-table中reahsh结束
    
    这里在rehash的过程中，每rehash一个bucket，就会吧old-data中的bucket数据删除

4. 然后把 table[0] 的数据释放掉， 把指针table[0] 指向table[1]的数据
    
<br>

## 2. 关于渐进式 rehash 中几个要点分析
----

1. rehash过程中，如果有插入，删除，修改数据怎么处理
   
    插入：只在new-table中进行，这样在之后的查找时，需要查找两个table

    修改：修改有点复杂，修改的操作是 查找 -> 更新, 其步骤如下:
    
      1. 先在老的表里边查找，如果找到了，说明数据还没有移动到new-table中，直接修改entry
    
      2. 如果没有找到，要么本来没有，要么已经移到new-table中了，那就执行插入操作
    
      3. 插入操作，如上所述直接在new-table中找
    
    删除：	删除的话，两个表一起同步删除

2. rehash的过程中会不会出现需要再次扩容的情况
   
    可以试想，如果在rehash开始后，一直是insert操作，那当insert的次数到达old-table的bucket个数时，rehash就完成了
    redis的扩容方式是2的倍数增长，所以应该不会出现，但是它代码里边倒是判断了两次扩容不能重叠进行